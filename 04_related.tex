\chapter{Long-range Modeling}
\label{chapter:related-long-range-modeling}


\renewcommand{\leftmark}{\spacedlowsmallcaps{Long-range Modeling}}

\ifthenelse{\boolean{skipRelated}}{\endinput}{}

\minitoc

\chapterwithfigures{\nameref*{chapter:related-long-range-modeling}}
\chapterwithtables{\nameref*{chapter:related-long-range-modeling}}

% Due to the ever-growing volume, it is difficult for humans to read, process, and extract vital and pertinent information from large-scale long texts. 

In real-world scenarios, long texts are a major information medium documenting human activities, \textit{e.g.}, academic articles, official reports, and meeting transcripts. Reading, analyzing, and extracting important information from large volumes of long texts poses a challenge for humans. Consequently, a compelling need arises for \ac{NLP} systems to model long texts and extract information of human interest. Broadly, the objective of long-range modeling is to capture salient semantics from text through informative representations, which hold utility for diverse downstream applications such as long document summarization \citep{cohan2018discourse, sharma2019bigpatent} and long question answering \citep{dasigi2021dataset}.
 
Handling long texts poses a challenge in \ac{NLP} due to the \textit{length limitation} imposed by Pre-trained Language Models on input sequences. Tokens exceeding the predefined maximum context length, typically shorter than the length of short texts, are discarded. A straightforward method involves truncating the input text to the pre-defined maximum length \citep{lewis2019bart}. Yet, with increasing length, salient information in a long document tends to be evenly distributed. Therefore, truncating the text may lead to substantial loss of information \citep{koh2022empirical}. Another approach consists in splitting the text into chunks, each fitting within the maximum length of the model. Each chunk is processed independently, and the resulting outputs are then aggregated. However, the model's receptive field is confined to a chunk, causing the disruption of long-range dependencies that extend across multiple chunks \citep{ding2020cogltx}. An alternative solution involves identifying and concatenating relevant sections of the text into a sequence, which is then processed by the model. However, employing a two-stage pipeline can lead to discrepancies between the two modules.
Therefore, modeling long texts remains an ongoing research problem that necessitates in-depth exploration.

Furthermore, computational efficiency cannot be overlooked. As the document's length increases, the time and memory requirements needed to model the text with standard Transformers increase quadratically, adding a substantial burden for practical applications. This high computational cost originates from various factors, with the computation of self-attention being a major contributor. In order to calculate the dot product $\bm{Q}\bm{K}^{\top} \in \mathbb{R}^{n \times n}$, the inner product of every single key with every single query must be computed, for each layer and each attention head. Given $n$ the sequence length, $h$ the number of attention heads, and $d$ the dimension of the representation space, the computational complexity for a self-attention operation on a single sequence is $\mathcal{O}(dn^2)$. The memory complexity to compute the attention matrix is $\mathcal{O}(dn + hn^2)$, the first term being the memory required to store keys and queries, and the second term referring to the scalar attention values produced by each head. Hence, the $\bm{Q}\bm{K}^{\top}$ matrix multiplication alone results in $n^2$ time and memory requirements, constraining the use of Transformers models to short sequences. Furthermore, the two feed-forward network components in each Transformer block also significantly contribute to the cost of Transformers. While having a linear complexity with respect to sequence length, feed-forward networks are still, in practice, resource-intensive.

Additionally, long document harbor distinct attributes when compared to shorter texts. As long texts are typically domain-specific documents with complex hierarchical structures, there is a need to consider long-range dependency (distant tokens may share semantic relationships with each other), inter-sentence relations (semantic connections between different sentences in the text), and discourse structure (long documents typically feature complex discourse structures, comprising sections and paragraphs) \citep{dong2023survey}.

In this chapter, we focus on modeling advances and architectural innovations that tackle the quadratic complexity issue of the self-attention mechanism. Furthermore, we explore benchmarks specifically designed for evaluating long-range modeling capabilities.

\section{Long-range Models}

To alleviate the cost of Transformers, a diversity of efficient self-attention model variants \citep{tay2020efficient} have been proposed over the past few years. Termed as \textit{Long-range Transformers}, these variants play a vital role in applications that model long sequences. Based on their core techniques and primary use case, long-range Transformers can be grouped into three categories \citep{qin2022nlp}: \textit{sparse patterns}, \textit{recurrence}, and \textit{low-rank and kernel} methods. 

% While the goal of most of these models is to improve the complexity of the self-attention mechanism, we also include methods that improve the general efficiency of the Transformer architecture. Most of these models can be used both as an encoder-only and an encoder-decoder model. 

%Enhancements made to self-attention are only applied at the encoder-level.

\subsection{Sparse Patterns}
\label{subsubsection:related-long-range-modeling-sparse}

The earliest modifications to self-attention involve applying pattern-based methods to sparsify the attention matrix, reducing it to a sparse version by computing attention solely on a limited number of query-key pairs. This approach restricts the field of view to specific patterns.
The key idea is to relax the constraint that a single layer is necessary to aggregate information from any two tokens. Although the attention of each layer is not dense, the receptive field can be increased as multiple layers are stacked. 

As an initial attempt, the Sparse Transformer \citep{child2019generating} introduces a two-dimensional factorization of the attention matrix, where the network can attend to all positions through two steps of sparse attention. Half of the attention heads attend only to preceding elements in the sequence, while the other half attend to predesignated indices spread evenly throughout the sequence. 

Longformer \citep{beltagy2020longformer}, a variant of Sparse Transformer, employs self-attention on both \textit{local} and \textit{global} contexts by introducing three attention patterns: \textit{sliding window attention}, \textit{dilated window attention}, and \textit{global attention}. The key concept underlying the first two patterns is similar to convolution: the most important information is supposedly contained in the neighbourhoods of the tokens \citep{liu2022leveraging}. \textit{Sliding window attention} constrains the field of view for each token to a $k$-sized window centered on the token. Although a token can only attend to itself and its neighbours in a single layer, sliding window attention enables a multi-layer Transformer to achieve a receptive field covering the entire sequence. Due to the need for multiple layers to incorporate information from the complete sequence, the sliding window can be dilated to increase the receptive field without increasing computation. With \textit{dilated window attention}, the window has gaps of size dilation $r$, resulting in a receptive field of $l \times r \times k$ where $l$ is the total number of layers. However, dilated sliding window attention alone does not suffice to produce task-specific representations: some tokens are so important that it is highly beneficial that each token is connected to them and conversely (\textit{e.g.}, through a single layer, the \texttt{[CLS]} token needs to have access to all input tokens for sequence classification tasks). \textit{Global attention} addresses this issue by allowing $s$ fixed, user-defined tokens to attend to every other token and vice-versa, enabling the model to learn task-specific representations. 

BigBird \citep{zaheer2020big} extends Longformer by adding \textit{random pattern attention}, which allows every token to attend to $r$ tokens chosen randomly, where $r$ is a small constant number. The intuition behind this mechanism is that the path lengths in a randomly connected graph are on average logarithmic. 

The \ac{ETC} model \citep{ainslie2020etc} represents another iteration within the Sparse Transformer family. It introduces a novel \textit{global-local} attention mechanism, encompassing four distinctive components: \textit{global-to-global}, \textit{global-to-local}, \textit{local-to-global}, and \textit{local-to-local} attentions. In addition to the original input, \ac{ETC} integrates $n_g$ auxiliary tokens at the beginning of the sequence, functioning as global tokens for participating in global-to-* and *-to-global attention processes. The local-to-local component operates as a localized attention mechanism with a predefined radius of $k$. Notably, \ac{ETC}'s approach closely resembles that of Longformer in its incorporation of global auxiliary tokens, which function as trainable parameters and can be interpreted as a form of model memory that pools across the sequence to collect global sequence information. 

The Sparse Transformer has a loglinear time and memory complexity, while Longformer, BigBird, and \ac{ETC} operates with a linear time and memory complexity. None of these models introduces new parameters beyond the Transformer model. Given the global attention mechanism, computing causal masks becomes unfeasible. As a result, the attention mechanisms employed by Longformer, BigBird, and \ac{ETC} are unsuitable for use in an autoregressive setting. Nevertheless, they can be effectively applied in sequence-to-sequence modeling, employed in the encoder while standard self-attention is retained in the decoder. Both Longformer and BigBird, as well as \ac{ETC}, have the capacity to handle up to 4,096 tokens.

% \subsubsection{Longformer}

% Longformer \citep{beltagy2020longformer} uses three patterns: \textit{sliding window attention} restricts each token's field of view to a local window, \textit{dilated window attention} makes each token only attend at fixed intervals, and \textit{global attention} allows some fixed, user-defined tokens to attend to every other token and vice-versa. The key concept underlying the first two patterns is similar to convolution: the most important information is supposedly contained in the neighbourhoods of the tokens. Thus, in one layer, a single token can only attend to itself and its neighbours. However, dilated sliding window attention alone does not suffice to produce task-specific representations: some tokens are so important that it is highly beneficial that each token is connected to them and conversely (\textit{e.g.}, through a single layer, the \texttt{[CLS]} token needs to have access to all input tokens for sequence classification tasks). Global attention addresses this issue by allowing the model to learn task-specific representations. Overall, the time and memory complexity of Longformer is $\mathcal{O}(2sn)$, where $s$ is the number of global tokens. 

% The pre-trained checkpoint for the encoder-only model has been trained using \ac{MLM} on sequences of 4,096 tokens extracted from long documents \citep{trinh2018simple, zellers2019defending}. Longformer was evaluated on three \ac{NLP} tasks: question answering \citep{welbl2018constructing, joshi2017triviaqa, yang2018hotpotqa}, coreference resolution \citep{pradhan2012conll}, and document classification \citep{maas2011learning, kiesel2019semeval}. 

% Given the global attention mechanism, computing causal masks becomes unfeasible. Consequently, Longformer's attention cannot be used in an autoregressive setting. However, it can still be leveraged for sequence-to-sequence modeling. \ac{LED}, a Longformer variant for supporting long document generative sequence-to-sequence tasks, was proposed for summarization. In this configuration, Longformer's attention is used in the encoder while standard self-attention is employed in the decoder. 

% A notable limitation of Longformer is its reliance on custom CUDA kernels to implement a block-sparse variance of matrix-matrix multiplication, rendering it impractical for use on TPUs.

% \subsubsection{BigBird}

% \citet{zaheer2020big} introduced BigBird, an extension to Longformer that adds a \textit{random pattern attention}, by which tokens can attend to any other tokens randomly. Each query attends to $r$ random keys, where $r$ is a small constant number, chosen randomly. The intuition behind this mechanism is that the path lengths in a randomly connected graph are on average logarithmic. BigBird has linear time and memory complexity, and does not introduce new parameters beyond the Transformer model. 

% BigBird has been evaluated on question answering \citep{yang2018hotpotqa, welbl2018constructing, kwiatkowski2019natural} and classification \citep{zhang2015character, maas2011learning, kiesel2019semeval, cohan2018discourse, sharma2019bigpatent} tasks. Similar to Longformer, BigBird's sparse attention cannot be used in an autoregressive fashion. However, it can be integrated in the encoder component of the original Transformer architecture for sequence-to-sequence tasks. The resulting model was evaluated on summarization tasks \citep{cohan2018discourse}. Furthermore, \citet{zaheer2020big} also leveraged BigBird's longer sequence capabilities for genomics applications \citep{dreos2013epd, zhou2015predicting}.

% \subsubsection{Reformer}

% Rather than employing fixed patterns, Reformer \citep{kitaev2020reformer} uses learnable patterns that enable the model to learn the access pattern in a data-driven fashion. Learnable patterns facilitates a more global view of the sequence while maintaining the efficiency benefits of fixed patterns approaches. Reformer introduces \ac{LSH} attention, a novel attention mechanism that consists in sharing parameters between $\bm{Q}$ and $\bm{K}$, and clustering tokens into chunks. This concept is rooted in the idea that if the sequence is long, $\text{softmax}(\bm{Q}\bm{K}^{\top})$ only puts significant weight on very few key vectors for each query vector. Hence, given a query $\bm{q}$, $\text{softmax}(\bm{qK})$ can be approximated by using only the keys that have a high cosine similarity with $\bm{q}$. If $\bm{K} = \bm{Q}$ then only the similarity of query vectors to each other has to be computed. Using the \ac{LSH} algorithm, query vectors are hashed into buckets of similar vectors. Attention is then computed among each bucket. If the bucket size is appropriately selected, the time and memory complexity of Reformer is $\mathcal{O}(n \log n)$. The model can easily be trained on sequences as long as 64,000 tokens. Reformer can be used in both bidirectional and autoregressive settings. 

% As a downstream application to evaluate Reformer, \citet{kitaev2020reformer} used image generation tasks \citep{parmar2018image}.

% \subsubsection{ETC}

% The \ac{ETC} model \citep{ainslie2020etc} represents another iteration within the Sparse Transformer family. It introduces a novel \textit{global-local} attention mechanism, encompassing four distinctive components: \textit{global-to-global}, \textit{global-to-local}, \textit{local-to-global}, and \textit{local-to-local} attentions. In addition to the original input, \ac{ETC} integrates $n_g$ auxiliary tokens at the beginning of the sequence, functioning as global tokens for participating in global-to-* and *-to-global attention processes. The local-to-local component operates as a localized attention mechanism with a predefined radius of $k$. Notably, \ac{ETC}'s approach closely resembles that of Longformer in its incorporation of global auxiliary tokens, which function as trainable parameters and can be interpreted as a form of model memory that pools across the sequence to collect global sequence information. The memory complexity of \ac{ETC} is $\mathcal{O}(n_g^2 + n_n N)$. 

% \ac{ETC} was evaluated on two \ac{NLP} tasks: question answering \citep{yang2018hotpotqa, welbl2018constructing, kwiatkowski2019natural} and information extraction \citep{xiong2019open}. 

% Because of the global attention mechanism, \ac{ETC} is not suitable for tasks where preserving the temporal order of information is essential, such as in sequence generation tasks. Furthermore, \ac{ETC} substantially increases code complexity, attributed to the addition of many attention directions.

% \subsection{Recurrence and Compressed Memory}
\subsection{Recurrence}

Recurrence and compressed memory approaches incorporate \textit{segment-level recurrence} into Transformer models to lengthen their attention span. The underlying concept of segment-based recurrence methods is to consider blocks of local receptive fields by chunking the input sequence into segments, and then process them in series via recurrence, as in \acp{RNN}.

% \subsubsection{Transformer-XL} 
Rather than attempting to reduce the cost of self-attention, \citet{dai2019transformer} take inspiration from \acp{RNN} and propose Transformer-XL, a causal language model that introduces a segment-based recurrence mechanism to connect adjacent segments. In Transformer-XL, segments are sequentially fed to the model, and tokens within a segment attend to the rest of the segment \textit{and} to the hidden states of the previous segment. Hence, after the first segment, tokens in subsequent segments will always have an immediate context size of $n$. By stacking multiple attention layers, the receptive field can be increased to multiple previous segments. In addition, this recurrence mechanism provides context for tokens in the beginning of a new segment. 
 
%Rather than treating the inputs as a sum of content and absolute position embeddings, each layer’s attention operation is broken up into a portion that attends based on content and a portion that attends based on relative position – for the 512th token in a chunk to attend to the 511th, the embedding corresponding to relative position -1 is used. Absolute position embeddings are only considered while computing attention weights, where they can be replaced with relative position embeddings.

% Transformer-XL introduces novel relative position encodings. In this scheme, absolute positional encodings are not added to the content embeddings. Instead, they are only considered while computing attention weights where they can be replaced with relative position encodings. S

% \subsubsection{XLNet}

XLNet \citep{yang2019xlnet} leverages both autoregressive and bidirectional language modeling. Unlike traditional autoregressive models that rely on fixed forward/backward factorization orders, XLNet maximizes the expected log likelihood of a sequence across all possible permutations of factorization orders. This approach allows each position in the sequence to consider tokens from both left and right, creating a bidirectional context. Additionally, XLNet incorporates the segment recurrence mechanism and relative encoding scheme of Transformer-XL during pre-training. This integration empirically improves the model's performance, specifically for tasks involving long text sequences. 

% XLNet was evaluated on several \ac{NLP} tasks involving reading comprehension \citep{lai2017race}, document ranking\footnote{\url{https://lemurproject.org/clueweb09/}}, question answering \citep{rajpurkar2016squad}, text classification \citep{maas2011learning, zhang2015character}, and \ac{NLU} \citep{wang2018glue}.

% \subsubsection{Compressive Transformers}

% In contrast to Transformer-XL, which entirely discards past activations as it moves across segments, Compressive Transformers \citep{rae2019compressive} retain a more detailed and fine-grained memory of previous segment activations. In this model, past activations are stored and compressed, contributing to a more effective capture of relevant information from earlier segments and its subsequent utilization in further processing. Hence, Compressive Transformers have access to a broader context and are able to capture longer-range dependencies across segments. 

% Instead of discarding past activations entirely, Compressive Transformers store and compress this information. This allows the model to preserve a broader context and capture longer dependencies across segments. By maintaining this compressed memory, Compressive Transformers can better capture relevant information from earlier segments and utilize it in subsequent processing, leading to improved contextual understanding and performance, especially for tasks requiring a strong grasp of distant dependencies.

\subsection{Approximating the Attention Mechanism}

Another approach to improve the efficiency of Transformer models is to approximate the self-attention mechanism through techniques such as \textit{learnable patterns}, \textit{low-rank approximation}, or \textit{kernelization}. The idea revolves around mathematically redefining the self-attention mechanism, which eliminates the need to explicitly compute the $n \times n$ matrix.

Reformer \citep{kitaev2020reformer} uses learnable patterns that enable the model to learn the access pattern in a data-driven fashion. Learnable patterns facilitates a more global view of the sequence while maintaining the efficiency benefits of fixed patterns approaches. Reformer introduces \ac{LSH} attention, a novel attention mechanism that consists in sharing parameters between $\bm{Q}$ and $\bm{K}$, and clustering tokens into chunks. This concept is rooted in the idea that if the sequence is long, $\text{softmax}(\bm{Q}\bm{K}^{\top})$ only puts significant weight on very few key vectors for each query vector. Hence, given a query $\bm{q}$, $\text{softmax}(\bm{qK})$ can be approximated by using only the keys that have a high cosine similarity with $\bm{q}$. If $\bm{K} = \bm{Q}$ then only the similarity of query vectors to each other has to be computed. Using the \ac{LSH} algorithm, query vectors are hashed into buckets of similar vectors. Attention is then computed among each bucket. If the bucket size is appropriately selected, the time and memory complexity of Reformer is $\mathcal{O}(n \log n)$. 
% Reformer can be used in both bidirectional and autoregressive settings. 

In a high-rank matrix, no particular dimension has much more information than any other. Conversely, most of the information in a low-rank matrix is concentrated in very few dimensions, meaning that most of the dimensions are redundant. The core idea behind Linformer \citep{wang2020linformer} is to approximate the self-attention matrix with a lower rank matrix. Given the dimensionality of the representation space $d$ and an integer $k < n$, Linformer projects the keys and values to a lower-dimensional representation $k$. The projected matrices $k \times d$ can be viewed as producing a set of $k$ pseudo-tokens that summarize the sequence — each of these pseudo-tokens indicates how highly a given filter activates on average when multiplied with the full sequence of corresponding representations. As $k$ and $d$ do not depend on the sequence length, the time and memory complexity of Linformer is linear. There is only a minimal parameter costs of the Linformer due to the extra $nk$ length projections. If $k$ is sufficiently small, there is negligible parameter costs incurred. 

% To evaluate Linformer, \citet{wang2020linformer} used sentiment classification \citep{maas2011learning, socher2013recursive}, \ac{NLI} \citep{wang2018glue} and textual similarity \citep{wang2017bilateral} tasks. 

% Because projecting on the length dimension $n$ causes mixing of sequence information, it is non-trivial to maintain causal masking and/or prevent mixing of past and future information when computing attention scores. Hence, Linformer's attention approximation cannot be used in an autoregressive setting.


To estimate standard full-rank-attention Transformers without relying on any prior such as sparsity or low-rankness, \citet{choromanski2020rethinking} generalize Linformer by proposing a kernel-based approach that uses a generalized attention framework to approximate any attention matrix. The attention matrix $\text{softmax}(\bm{Q}\bm{K}^{\top})$ can be approximated using lower-rank randomized matrices $\bm{Q'}$ and $\bm{K'}$ where the rows encode positive-valued nonlinear functions of the original $\bm{Q}$ and $\bm{K}$. This approximation allows to store the implicit attention matrix $\bm{A}$ with linear memory complexity. Additionally, this framework facilitates the creation of a diverse range of attention mechanisms by leveraging various similarity measures (kernels). 
% To obtain a linear time complexity, matrix multiplications are rearranged: instead of multiplying $\bm{A}$ with $\bm{V}$ to obtain the final $n \times d$ matrix, $\bm{K'}^{\top} \in \mathbb{R}^{k \times n}$ is first multiplied with $\bm{V} \in \mathbb{R}^{n \times d}$, and $\bm{Q'} \in \mathbb{R}^{n \times k}$ is multiplied with the resulting matrix $\bm{K'}^{\top} \bm{V} \in \mathbb{R}^{k \times d}$. This framework allows to create a broad class of attention mechanisms based on different similarity measures (kernels). 

% Performer was evaluated as an encoder-only model on protein modeling \citep{uniprot2019uniprot} and image generation \citep{parmar2018image} tasks. 

Reformer can be used in both bidirectional and autoregressive settings. In Linformer, projection along the length dimension $n$ induces the mixing of sequence information. Similarly, the randomized feature map and the approximations involved in the kernel-based approach might not inherently preserve the sequential order required for causal masking. Therefore, it is non-trivial to maintain causal masking and/or prevent mixing of past and future information when computing attention in both Linformer and Performer. Consequently, neither Linformer's nor Performer's attention approximations are suitable for deployment in an autoregressive setting.

% The model poses considerable challenges in meeting the requirements for achieving causal masking. 


\section{Benchmarking Long-range Models}

The broad array of efficient Transformers that has emerged to address the challenge of long-range modeling have been mostly evaluated using differents sets of downstream tasks and datasets. Longformer and BigBird were evaluated on question answering \citep{yang2018hotpotqa, welbl2018constructing} and text classification \citep{maas2011learning, kiesel2019semeval}. \ac{ETC}'s evaluation extends to information extraction tasks \citep{xiong2019open}. XLNet's performance is assessed using machine reading comprehension \citep{lai2017race} and document ranking\footnote{\url{https://lemurproject.org/clueweb09/}}. Linformer is evaluated on easier tasks such as sentiment classification \citep{maas2011learning, socher2013recursive}, \ac{NLI} \citep{wang2018glue} and textual similarity \citep{wang2017bilateral}, where simpler models such as \acp{CNN} perform well. On the other hand, the Sparse Transformer, Reformer, and Performer have their performance assessed on image generation tasks \citep{parmar2018image}. The large diversity of tasks and datasets used complicates the comparison of models and the assessment of their relative strengths and weaknessses. 

Furthermore, intrinsic metrics such as perplexity or \ac{BPC} are widely used to assess the performance of efficient Transformers. However, an increasing amount of literature shows that predicting the next token is mostly a local task that does not require modeling long-range dependencies \citep{khandelwal2018sharp, sun2021long}. 

In this section, we describe unified benchmarks designed to shed light on the ability of these architectures to reason in long-context scenarios, and study their performance in handling \ac{NLP} tasks.

\subsection{Long-Range Benchmarks}

\subsubsection{Long-Range Arena} \citet{tay2020long} introduce a systematic and unified benchmark, \ac{LRA}, designed to evaluate the ability of a model to reason in long-context scenarios. This benchmark is a suite of tasks with sequences ranging from 1,000 to 16K tokens, encompassing various data types and modalities (text, natural and synthetic images, mathematical expressions). This benchmark was created based on a set of specific requirements and criteria. First, \ac{LRA} is general: all long-range Transformer models are applicable to the tasks. The tasks are intentionally designed to promote simple models rather than cumbersome pipelined approaches. Furthermore, the tasks are crafted to be challenging enough to ensure there is room for improvement. The input sequences are reasonably long, and the set of tasks allows to assess different capabilities of models. Finally, \ac{LRA} is deliberately non-resource intensive and accessible. 

\ac{LRA} contains five classification tasks. In \textit{Long ListOps}, sequences with a hierarchical structure and mathematical operators are given as input and the model has to predict the mathematical result of the sequence as a classification task. The goal is to evaluate the ability to model hierarchically structured data while handling long contexts. 
In the \textit{Character-level Sentiment Analysis} task, the model is provided with character-level IMDb reviews \citep{maas2011learning} and has to classify them into positive or negative. This task benchmarks the ability of the model to deal with compositionality as it is required to compose characters into words, and words into higher-level phrases.
Given two documents from the ACL Anthology Network \citep{radev2013acl} represented as character-level sequences, the \textit{Character-level Document Relatedness} task consists in predicting whether these documents are related. This task assesses the capability of a model to compress long sequences into representations suitable for similarity-based matching. As previously, the character level setup challenges the model to compose and aggregate information over long contexts.
Using the CIFAR-10 dataset \citep{krizhevsky2009learning}, the \textit{Image Classification on sequences of pixels} task requires the model to learn the 2D spatial relations between input pixels, while presented as a 1D sequence of symbols.
In \textit{Pathfinder}, the model is given a sequence of pixels and has to predict whether two points are connected by a path. A more challenging version with extreme lengths, \textit{Pathfinder-X}, evaluates if the same algorithmic challenges bear a different extent of diffculty when sequence lengths are much longer.

However, only two out of these five tasks involve natural language, restricting the relevance of \ac{LRA} for \ac{NLP}. Furthermore, \ac{LRA} artificially increases the length of natural language sequences through character tokenization, and truncates each example at 4,000 characters, equivalent to less than 1,000 words. This exempts models from dealing with the complex long-range interactions present in long texts.
% The tasks in the \ac{LRA} benchmark are specifically designed for the purpose of probing different aspects of long-range Transformer models. 

\subsubsection{SCROLLS} To extend evaluation beyond sentences and paragraphs, \citet{shaham2022scrolls} propose \ac{SCROLLS}, an \ac{NLP} benchmark featuring datasets with long texts, ranging from an average of 1,700 words to over 51,000. \ac{SCROLLS} consists of seven datasets covering various domains and tasks: summarization from government reports (\textit{GovReport}), TV shows transcripts (\textit{SummScreenFD}), and meetings (\textit{QMSum}), question answering from scientific articles (\textit{Qasper}), books (\textit{NarrativeQA}), and stories (\textit{QuALITY}), and \ac{NLI} from non-disclosure agreements (\textit{Contract NLI}). Every task is reformulated as a sequence-to-sequence problem for a simple unified format. \ac{SCROLLS} presents a challenge not only due to the length of its inputs but also because it requires models to process long-range interactions across different sections. 

\subsection{On the Effectiveness of Long-range Models for NLP Tasks}

To validate the effectiveness and long-range ability of long-range Transformers on language tasks and uncover the underlying factors behind model behaviors, \citet{qin2022nlp} benchmark different long-range Transformer models for \ac{NLP} tasks characterized by long sequences. Five complex, long-text \ac{NLP} tasks are considered, covering a wide spectrum of typical language scenarios: token/span-level prediction, sequence-level classification, and sequence-to-sequence generation.

\subsubsection{Sparse Pattern Models}

Longformer and BigBird are used to assess the performance of sparse pattern approaches. In coreference resolution, which consists in identifying mention spans and clustering them into entities, \citet{qin2022nlp} find that using larger sliding windows can be advantageous, but this advantage tends to level off or even decline after a certain point. In tasks where the quantity of guiding text, \textit{i.e.}, specific textual information provided explicitly to guide the model's behavior, is limited—such as questions in question answering—designating it as global tokens can enhance attention and substantially improve overall performance. When there is no guiding text (\textit{e.g.}, in the case of coreference resolution), setting all tokens as global reintroduces quadratic complexity. Additionally, \citet{qin2022nlp} find a connection between long-range attention, global tokens, and the selectivity of sequence-to-sequence problems, which ultimately enhances the decoding process. Globally, the authors show that pattern-based methods, despite being a widely adopted approach, do not effectively capture long distance information.

\subsubsection{Recurrence Models} 

The effectiveness of recurrence-based methods is evaluated using XLNet. In various tasks, \citet{qin2022nlp} show that the memory of recurrence models tends to enhance performance, demonstrating the advantage of using past hidden states in Transformers. Nevertheless, XLNet falls short in maximizing the potential of past tokens, as it gives relatively less attention to distant information. This could be attributed to XLNet's pretraining objective of predicting masked tokens, which does not consistently require long-range context \citep{sun2021long}. Moreover, the application of the stop-gradient technique might impede the model's ability to efficiently focus on memories.

\subsubsection{Kernel-based Models} 

Performer is used as a kernel-based model. It is found that the approximation technique of Performer demonstrates strong performance with shallow networks. However, when applied to deeply stacked Transformer layers, it encounters significant  error accumulation issues. This leads to a notable drop in performance, which is considered unacceptable even for the base version of Transformer encoders. 


\subsubsection{Model Selection Takeaways} 

Drawing from their discoveries, \citet{qin2022nlp} offer a few recommendations. For typical tasks like sequence classification or token-level prediction, it remains effective to divide inputs into chunks and use short-range Transformer models. In cases where explicit guiding text such as queries is available, models based on sparse patterns and featuring a global token mechanism are preferable. For sequence-to-sequence problems, leveraging long-range Transformers with pre-trained checkpoints yields superior performance. \\

The exploration of long-range modeling has been marked by continuous efforts to reduce the cost of Transformers to efficiently model long texts. The quest for ideal long-range models demands finding an equilibrium. These models should address the quadratic issue of Transformers, showcase universality by performing well across most tasks, and remain simple without unnecessary hard-coding or engineering complexities. There should be no compromise between speed and memory efficiency, and they should be able to seamlessly integrate with TPUs and accomodate causal masking. 

\acresetall
