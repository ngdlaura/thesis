\chapter{Introduction}
\label{chapter:introduction}

% \minitoc
\chapterwithfigures{\nameref*{chapter:introduction}}
\chapterwithtables{\nameref*{chapter:introduction}}

\ifthenelse{\boolean{skipIntro}}{\endinput}{}

% 1. Opening paragraph: grab the reader's attention

Driven by factors such as increased digitization, the adoption of electronic communication, and the expansion of online platforms, the past two decades have seen an unprecedented and ever-increasing trend in data production \citep{hilbert2011world, clissa2022survey}. Notably, it is estimated that the stock of textual data is currently expanding at a rate of 7\% per year \citep{villalobos2022will}. In a world saturated with information, where the volume of electronic documents keeps growing, the ability of machines to \textit{read, understand and interpret documents}\footnote{\url{https://document-intelligence.github.io/DI-2022/}} with human-like proficiency becomes increasingly crucial. This question forms the core focus of our investigation into improving the automated process of reading, interpreting, and extracting meaningful information from documents, \textit{i.e.}, \emph{Document Understanding}.

% 2. Background and context: establish the problem space
%   Provide a brief overview of the broader field of document understanding and its significance. 
%   Introduce the challenges and complexities associated with traditional methods of document analysis and comprehension.
%   Discuss the increasing importance of effective document understanding in various domains and industries. 

The foundation of digital transformation lies in automated information processing, with escalating demands for increased processing power, speed, and accuracy across multiple domains and industries such as law, business, and healthcare. In the business field, electronic documents play a central role. Ranging from purchase receipts and industry reports to sales contracts and financial statements, business documents encapsulate a plethora of complex information. In this context, hyperautomation has emerged as a highly-demanded\footnote{\url{https://www.gartner.com/en/documents/4019586}} approach to automate and optimize business document processing by introducing \ac{AI} technologies. Examples of successful products that have empowered a range of industries with hyperautomation technologies include Microsoft Azure AI Document Intelligence,\footnote{\url{https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence/}} Amazon Textract,\footnote{\url{https://aws.amazon.com/fr/textract/}} and Google Document AI,\footnote{\url{https://cloud.google.com/document-ai?hl=en}} among others. At the core of hyperautomation lies Document Understanding, also known as \textit{Document Intelligence}. Encompassing the techniques used to automatically analyze and understand documents, Document Understanding is challenging due to the complex structures and varied formats of documents, the quality of scans and \ac{OCR} systems used, and the diversity of knowledge domains.

In contrast to mainstream \ac{NLP}, which typically deals with plain text documents, the content encountered in real-world documents—such as scientific articles, news reports, or emails—is rarely strictly sequential. When creating documents, writers rely on the assumption that structured and visual formats, such as tables, graphs, or infographics, are more understandable to readers than sequential text. This preference is grounded in human visual perception and our ability to comprehend spatial context within a text. In other words, the appearance of a document, and especially its layout—defined as the spatial arrangement and organization of both graphical and textual elements—guides our reading process and conveys rich semantic information. Conventional \ac{NLP} approaches do not consider this information, which may hinder their effectiveness in handling real-world documents. This underscores the critical need for document understanding systems to incorporate multimodal information, extending beyond mere textual content to encompass document layout and visual appearance.

Addressing the challenges posed by Document Understanding requires a \textit{multi-disciplinary} approach that extends beyond \ac{NLP}, encompassing fields such as Computer Vision to handle visual elements and Information Retrieval for document search and clustering. Over the past thirty years, document understanding systems have incorporated a fundamental aspect of \textit{multimodality}, evolving from rule-based heuristics and Machine Learning approaches to methods based on \textit{Deep Learning}. Notably, a significant breakthrough in the field has been observed with the widespread adoption of large-scale multimodal pre-training for general document-level understanding \citep{xu2020layoutlm}. This versatile framework produces powerful \textit{language models} with general knowledge, making them easily adaptable for various applications. Large-scale pre-training has become a cornerstone for document understanding systems, leading to a remarkable leap in performance across various tasks in the field. Yet, despite its importance for digitization, the academic literature on resources and methodologies for addressing Document Understanding remains relatively scarce. Furthermore, the field faces critical limiting factors for achieving satisfying results in practical applications. 

A key challenge lies in the constraint on the input length of current large-scale pre-trained language models, restricted to a maximum of 1,024 tokens. This limitation hinders effective multi-page and cross-page understanding of long and complex documents. Furthermore, the discrepancy between high-quality annotated training data and real-world documents, commonly obtained from scanning equipment, and exhibiting lower quality, can result in sub-optimal performance. Besides, document understanding systems face challenges in practical applications due to insufficient computing resources and labeled training samples. In addition, existing document understanding tasks are often treated independently, lacking effective leveraging of correlations between tasks. Finally, accuracy in text recognition and word ordering (\textit{i.e.}, serialization) from \ac{OCR} and PDF-parsing engines plays a pivotal role in downstream tasks, sometimes even more than the choice of model architecture \citep{borchmann2021due}. \\

Aligning with the current trend in the field, this thesis explores the use of Deep Learning methods, with a particular emphasis on general-purpose pre-training techniques, to advance automatic document understanding. Within the scope of this thesis, we cover two key challenges in the field: 1) Developing \textit{efficient} and \textit{effective} methods to encode the \textit{multimodal} nature of documents, and 2) Formulating strategies to efficiently and effectively process \textit{long and complex} documents, with consideration for their \textit{visual appearance}.

% 1) Developing efficient methods for processing \textit{long and complex} documents with consideration for their \textit{visual appearance}. 2) Establishing \textit{robustness} against \ac{OCR}-induced \textit{serialization errors}, particularly when dealing with visually-rich documents with \textit{complex layouts}.

The organization of this PhD thesis is as follows: Chapters~\ref{chapter:related-language-modeling} to~\ref{chapter:related-document-understanding} review existing literature and works relevant to our research, while Chapters~\ref{chapter:skim-attention} to~\ref{chapter:loralay} present our contributions to the field.

\paragraph{Literature review} Chapter~\ref{chapter:related-language-modeling} explores the extensive literature that forms the foundation of Language Models, tracing their evolution from the early era of Statistical Language Models to the emergence of Neural Language Models. Chapter~\ref{chapter:related-pretrained-language-models} centers on the Transformer architecture \citep{vaswani2017attention} and its application in creating powerful Pre-trained Language Models, unraveling how these foundational models have reshaped the landscape of \ac{NLP}. However, the applicability of these models to long documents is hindered by the quadratic complexity of the Transformer. This complexity primarily stems from its core component, the self-attention mechanism, which necessitates each element in the sequence to acquire information from every other element in the sequence. Therefore, Chapter~\ref{chapter:related-pretrained-language-models} also delves into modeling advances and architectural innovations that tackle the complexity issue of self-attention. Closer to the applicative field of this PhD thesis, Chapter~\ref{chapter:related-document-understanding} offers an overview of the field of Document Understanding, exploring recent advancements driven by Transformer-based pre-training techniques for deep fusion of modalities. 

\paragraph{Leveraging layout-only information} We address the research question of developing efficient and effective methods to encode the multimodal nature of documents. Our strategy involves designing approaches that rely only on layout information to build meaningful representations, aiming to enhance both the effectiveness and efficiency of document understanding processes. As existing multimodal pre-trained models for Document Understanding are developed without sufficient attention to efficiency considerations, they still grapple with the quadratic complexity of self-attention. Incorporating layout information, while beneficial, increases resource demands compared to models that exclusively handle text. Chapter~\ref{chapter:skim-attention} (\textit{Leveraging layout for sparse attention}) focuses on \textit{exploiting layout in a computationally efficient manner} by investigating 1) whether attention can be determined solely from layout information, and 2) whether layout can contribute to mitigating the complexity of self-attention. Drawing inspiration from insights in cognitive science, we address these questions by introducing \textit{Skim-Attention}, a novel attention mechanism exclusively based on document layout. This approach not only offers a novel means to integrate layout information for enhanced performance but also enables the sparsification of attention.

% Drawing inspiration from insights in cognitive science, we address these questions by introducing \textit{Skim-Attention}, a novel attention mechanism exclusively based on document layout, allowing to mirror human reading strategies and sparsify attention. 

Furthermore, we introduce a strategy based solely on layout to address and avoid reader order issues. While layout inherently encapsulates the correct reading order of documents through visually organized content, existing pre-training methods for Document Understanding often overlook this aspect, relying solely on \ac{OCR} or PDF parsing to determine the reading order of documents. Yet, accurately retrieving the reading order of visually-rich documents proves challenging, potentially compromising performance in downstream tasks. In Chapter~\ref{chapter:layout2pos} (\textit{Leveraging layout to avoid reading order issues}), we investigate whether discarding reading order information obtained via \ac{OCR} and, instead, \textit{strategically depending on layout information to establish an alternative for the provided reading order of documents} can sustain performance in visual information extraction tasks. Based on the intuition that layout contains the information to supplement reading order, we present \textit{Layout2Pos}—a shallow Transformer designed to learn position embeddings from layout exclusively. By avoiding reading order issues linked to the use of sequential position information, this design choice enhances the robustness of models to changes in reading order.

\paragraph{Processing long documents with awareness of their layout} 
We explore the potential of leveraging layout to improve the performance of models for tasks associated with long and complex documents. The significance of document structure for information processing, as demonstrated by prior works in cognitive sciences and particularly crucial in long documents, emphasizes the need for efficient modeling of layout information. However, methods for efficient long document understanding remain under-explored. In Chapter~\ref{chapter:loralay} (\textit{Leveraging layout to deal with long and layout-rich documents}), we investigate the significance of \textit{integrating layout information into long-range modeling} for Long Text Summarization. This task, well-suited to benefit from a global context, relies on document structures to guide summary generation. To spurr further research on how to incorporate multimodal information to better capture long-range dependencies, we build \textit{LoRaLay}, a multilingual corpus of datasets designed for long document summarization with consideration for their visual appearance. These datasets, enriched with layout/visual information, along with novel baselines combining layout-aware and long-range models, provide valuable resources for exploring the use of multimodal information in long document modeling. 

By developing strategies relying exclusively on layout information, we contribute to the efficient and effective encoding of the multimodal nature of documents, offering practical advancements in Document Understanding. Additionally, our investigations into leveraging layout for processing long and complex documents provide valuable resources and insights to efficiently and effectively address the challenges associated with these documents. These contributions offer novel solutions to challenges in Document Understanding, augmenting the practical utility of document understanding models.

% The research undertaken in addressing these limitations holds significant potential impact. Enhancing the efficiency of processing long and complex documents can benefit sectors relying on extensive document analysis, such as legal, healthcare, and research. Robustness against serialization errors is crucial for maintaining the integrity of extracted information, especially in visually-rich documents. 

% Therefore, documents come in a variety of layouts to cater to diverse sets of information for varied audiences. Therefore, understanding documents with rich layouts plays a vital role in digitization and hyper-automation.

% Language serves as a sophisticated means of human expression, guided by complex grammatical rules. Individuals harness language to compose documents, thereby facilitating the transmission and preservation of information.

% Language is essentially a complex, intricate system of human expressions governed by grammatical rules. 

% Humans compose documents to record and preserve information. As information carrying vehicles, documents are written using different layouts to represent diverse sets of information for a variety of different consumers.

% To effectively convey information to readers, writers commonly rely on the assumption that structured formats like tables, graphs, or infographics are more comprehensible than sequential text. This is attributed to human visual perception and our capacity to grasp a text's spatial context.

% 3. Identify the research gap
%   Highlight the limitations or gaps in existing approaches to document understanding.
%   Discuss why current methods may fall short and emphasize the need for advanced techniques



% 4. State the Research Problem
%   Clearly articulate the main research question or problem your thesis aims to address.
%   Define the specific objectives of your research and the contributions it intends to make to the field of document understanding.

% 5. Justify the research
%   Explain the potential impact and significance of your research in addressing the identified gap.
%   Discuss how advancements in document understanding can have practical applications and benefit various sectors.

% 6. Scope and Limitations
%   Clearly define the scope of your study, specifying what aspects of document understanding your research will cover.
%   Acknowledge any limitations or constraints that might affect the generalizability of your findings.

% 7. Research methodology 

% 8. Outline the structure


% draw inspi from due paper

% The layout of a document, which refers to the arrangement and organization of its visual and textual elements,

% draw inspi from DOCUMENT AI: BENCHMARKS, MODELS AND APPLICATIONS

% Modeling long texts has been an essential technique in the field of natural language processing (NLP).