@string{ jmlr = "Journal of Machine Learning Research (JMLR)" }
@string{ tit = "IEEE Transactions on Information Theory" }
@string{ tip = "IEEE Transactions on Image Processing (TIP)" }
@string{ tnn = "IEEE Transactions on Neural Networks" }
@string{ pami = "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)" }
@string{ ijcv = "International Journal of Computer Vision (IJCV)" }
@string{ tcs = "Theoretical Computer Science" }
@string{ neuralcomp = "Neural Computation" }
@string{ mathprog = "Mathematical Programming" }
@string{ ai = "Artificial Intelligence" }
@string{ ml = "Machine Learning" }
@string{ cvpr = "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)" }
@string{ cvprw = "IEEE Conference on Computer Vision and Pattern Recognition Workshop (CVPR-W)" }
@string{ nips = "Advances in Neural Information Processing Systems (NIPS)" }
@string{ neurips = "Advances in Neural Information Processing Systems (NeurIPS)" }
@string{ neuripssub = "Under Review at Advances in Neural Information Processing Systems (NeurIPS)" }
@string{ nipsw = "Advances in Neural Information Processing Systems Workshop (NIPS-W)" }
@string{ icml = "International Conference on Machine Learning (ICML)" }
@string{ icmlw = "International Conference on Machine Learning Workshop (ICML-W)" }
@string{ ecml = "European Conference on Machine Learning (ECML)" }
@string{ eccv = "European Conference on Computer Vision (ECCV)" }
@string{ eccvw = "European Conference on Computer Vision Workshop ({ECCV-W})" }
@string{ bmvc = "{British Machine Vision Conference (BMVC)}" }
@string{ aistats = "International Conference on Artificial Intelligence and Statistics (AISTATS)" }
@string{ iclr = "International Conference on Learning Representations (ICLR)" }
@string{ ijcai = "International Joint Conferences on Artificial Intelligence (IJCAI)" }
@string{ iclrw = "International Conference on Learning Representations Workshop (ICLR-W)" }
@string{ icip = "IEEE International Conference on Image Processing (ICIP)" }
@string{ iccv = "IEEE International Conference on Computer Vision (ICCV)" }
@string{ cviu = "Computer Vision and Image Understanding (CVIU)" }
@string{ tnnls = "IEEE Transactions on Neural Networks and Learning Systems (TNNLS)" }
@string{ aaai = "Conference on Artificial Intelligence (AAAI)" }
@string{ sigir = "Special Interest Group on Information Retrieval (SIGIR)" }
@string{ arxiv = "arXiv preprint libary" }

@string{ esann = "European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)"}

@article{britton1982effects,
  title={Effects of text structure on use of cognitive capacity during reading.},
  author={Britton, Bruce K and Glynn, Shawn M and Meyer, Bonnie J and Penland, MJ},
  journal={Journal of Educational Psychology},
  volume={74},
  number={1},
  pages={51},
  year={1982},
  publisher={American Psychological Association}
}

@article{katz1987estimation,
  title={Estimation of probabilities from sparse data for the language model component of a speech recognizer},
  author={Katz, Slava},
  journal={IEEE transactions on acoustics, speech, and signal processing},
  volume={35},
  number={3},
  pages={400--401},
  year={1987},
  publisher={IEEE}
}

@article{lecun1989backpropagation,
  title={Backpropagation applied to handwritten zip code recognition},
  author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal={Neural computation},
  volume={1},
  number={4},
  pages={541--551},
  year={1989},
  publisher={MIT Press}
}

@article{bahl1989tree,
  title={A tree-based statistical language model for natural language speech recognition},
  author={Bahl, Lalit R and Brown, Peter F and de Souza, Peter V and Mercer, Robert L},
  journal={IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume={37},
  number={7},
  pages={1001--1008},
  year={1989},
  publisher={IEEE}
}

@article{werbos1990backpropagation,
  title={Backpropagation through time: what it does and how to do it},
  author={Werbos, Paul J},
  journal={Proceedings of the IEEE},
  volume={78},
  number={10},
  pages={1550--1560},
  year={1990},
  publisher={IEEE}
}

@inproceedings{lebourgeois1992fast,
  title={A fast and efficient method for extracting text paragraphs and graphics from unconstrained documents},
  author={Lebourgeois, Frank and Bublinski, Zbigniew and Emptoz, Hubert},
  booktitle={11th IAPR International Conference on Pattern Recognition. Vol. II. Conference B: Pattern Recognition Methodology and Systems},
  volume={1},
  pages={272--273},
  year={1992},
  organization={IEEE Computer Society}
}

@article{brown1992class,
  title={Class-based n-gram models of natural language},
  author={Brown, Peter F and Della Pietra, Vincent J and Desouza, Peter V and Lai, Jennifer C and Mercer, Robert L},
  journal={Computational linguistics},
  volume={18},
  number={4},
  pages={467--480},
  year={1992}
}

@article{marcus1993building,
  title={Building a large annotated corpus of English: The Penn Treebank},
  author={Marcus, Mitchell and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  year={1993}
}

@article{gage1994new,
  title={A new algorithm for data compression},
  author={Gage, Philip},
  journal={C Users Journal},
  volume={12},
  number={2},
  pages={23--38},
  year={1994},
  publisher={McPherson, KS: R \& D Publications, c1987-1994.}
}

@article{jones1994natural,
  title={Natural language processing: a historical review},
  author={Jones, Karen Sparck},
  journal={Current issues in computational linguistics: in honour of Don Walker},
  pages={3--16},
  year={1994},
  publisher={Springer}
}

@article{gale1995good,
  title={Good-turing frequency estimation without tears},
  author={Gale, William A and Sampson, Geoffrey},
  journal={Journal of quantitative linguistics},
  volume={2},
  number={3},
  pages={217--237},
  year={1995},
  publisher={Taylor \& Francis}
}

@book{kress1996reading,
  title={Reading images: The grammar of visual design},
  author={Kress, Gunther R and Van Leeuwen, Theo and others},
  year={1996},
  publisher={Psychology Press}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}

@article{wright1999psychology,
  title={The psychology of layout: Consequences of the visual structure of documents},
  author={Wright, Patricia},
  journal={American Association for Artificial Intelligence Technical Report FS-99-04},
  pages={1--9},
  year={1999}
}

@incollection{ramshaw1999text,
  title={Text chunking using transformation-based learning},
  author={Ramshaw, Lance A and Marcus, Mitchell P},
  booktitle={Natural language processing using very large corpora},
  pages={157--176},
  year={1999},
  publisher={Springer}
}

@inproceedings{thede1999second,
  title={A second-order hidden Markov model for part-of-speech tagging},
  author={Thede, Scott M and Harper, Mary},
  booktitle={Proceedings of the 37th annual meeting of the Association for Computational Linguistics},
  pages={175--182},
  year={1999}
}

@article{rosenfeld2000two,
  title={Two decades of statistical language modeling: Where do we go from here?},
  author={Rosenfeld, Ronald},
  journal={Proceedings of the IEEE},
  volume={88},
  number={8},
  pages={1270--1278},
  year={2000},
  publisher={IEEE}
}

@article{bengio2000neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal},
  journal={Advances in neural information processing systems},
  volume={13},
  year={2000}
}

@inproceedings{petrushin2000hidden,
  title={Hidden markov models: Fundamentals and applications},
  author={Petrushin, Valery A},
  booktitle={Online Symposium for Electronics Engineer},
  year={2000}
}

@misc{hochreiter2001gradient,
  title={Gradient flow in recurrent nets: the difficulty of learning long-term dependencies},
  author={Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, J{\"u}rgen and others},
  year={2001},
  publisher={A field guide to dynamical recurrent neural networks. IEEE Press In}
}

@article{hofmann2001unsupervised,
  title={Unsupervised learning by probabilistic latent semantic analysis},
  author={Hofmann, Thomas},
  journal={Machine learning},
  volume={42},
  pages={177--196},
  year={2001},
  publisher={Springer}
}

@article{amin2001page,
  title={Page segmentation and classification utilizing bottom-up approach},
  author={Amin, Adnan and Shiu, Ricky},
  journal={International Journal of Image and Graphics},
  volume={1},
  number={02},
  pages={345--361},
  year={2001},
  publisher={World Scientific}
}

@article{blei2003latent,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={Journal of machine Learning research},
  volume={3},
  number={Jan},
  pages={993--1022},
  year={2003}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@inproceedings{dang2005overview,
  title={Overview of DUC 2005},
  author={Dang, Hoa Trang},
  booktitle={Proceedings of the document understanding conference},
  volume={2005},
  pages={1--12},
  year={2005}
}

@article{jones2005some,
  title={Some points in a time},
  author={Jones, Karen Sp{\"a}rck},
  journal={Computational Linguistics},
  volume={31},
  number={1},
  pages={1--14},
  year={2005},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@inproceedings{lewis2006building,
  title={Building a test collection for complex document information processing},
  author={Lewis, David and Agam, Gady and Argamon, Shlomo and Frieder, Ophir and Grossman, D and Heard, Jefferson},
  booktitle={Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={665--666},
  year={2006}
}

@article{kendeou2007effects,
  title={The effects of prior knowledge and text structure on comprehension processes during reading of scientific texts},
  author={Kendeou, Panayiota and Van Den Broek, Paul},
  journal={Memory \& cognition},
  volume={35},
  number={7},
  pages={1567--1577},
  year={2007},
  publisher={Springer}
}

@article{kay2007tesseract,
    author = {Kay, Anthony},
    title = {Tesseract: An Open-Source Optical Character Recognition Engine},
    year = {2007},
    issue_date = {July 2007},
    publisher = {Belltown Media},
    address = {Houston, TX},
    volume = {2007},
    number = {159},
    issn = {1075-3583},
    abstract = {If you really need OCR.},
    journal = {Linux J.},
    month = Jul,
    pages = {2}
}

@article{schwenk2007continuous,
  title={Continuous space language models},
  author={Schwenk, Holger},
  journal={Computer Speech \& Language},
  volume={21},
  number={3},
  pages={492--518},
  year={2007},
  publisher={Elsevier}
}

@inproceedings{collobert2008unified,
  title={A unified architecture for natural language processing: Deep neural networks with multitask learning},
  author={Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={160--167},
  year={2008}
}

@article{toprak2009three,
  title={Three reading phases and their applications in the teaching of english as a foreign language in reading classes with young learners},
  author={Toprak, Elif and Almacio{\u{g}}lu, Gamze},
  journal={Journal of language and Linguistic Studies},
  volume={5},
  number={1},
  year={2009}
}

@inproceedings{mikolov2010recurrent,
  title={Recurrent neural network based language model.},
  author={Mikolov, Tomas and Karafiat, Martin and Burget, Lukas and Cernocky, Jan and Khudanpur, Sanjeev},
  booktitle={Interspeech},
  volume={2},
  number={3},
  pages={1045--1048},
  year={2010},
  organization={Makuhari}
}

@article{collobert2011natural,
  title={Natural language processing (almost) from scratch},
  author={Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
  journal={Journal of machine learning research},
  volume={12},
  number={ARTICLE},
  pages={2493--2537},
  year={2011}
}

@inproceedings{collobert2011deep,
  title={Deep learning for efficient discriminative parsing},
  author={Collobert, Ronan},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={224--232},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

@inproceedings{arisoy2012deep,
  title={Deep neural network language models},
  author={Arisoy, Ebru and Sainath, Tara N and Kingsbury, Brian and Ramabhadran, Bhuvana},
  booktitle={Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT},
  pages={20--28},
  year={2012}
}

@article{leckner2012presentation,
  title={Presentation factors affecting reading behaviour in readers of newspaper media: an eye-tracking perspective},
  author={Leckner, Sara},
  journal={Visual Communication},
  volume={11},
  number={2},
  pages={163--184},
  year={2012},
  publisher={Sage Publications Sage UK: London, England}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{kim2014convolutional,
  title={Convolutional neural networks for sentence classification},
  author={Kim, Yoon},
  journal={arXiv preprint arXiv:1408.5882},
  year={2014}
}

@article{cho2014properties,
  title={On the properties of neural machine translation: Encoder-decoder approaches},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.1259},
  year={2014}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{kalchbrenner2014convolutional,
  title={A convolutional neural network for modelling sentences},
  author={Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
  journal={arXiv preprint arXiv:1404.2188},
  year={2014}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{bowman2015large,
  title={A large annotated corpus for learning natural language inference},
  author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},
  journal={arXiv preprint arXiv:1508.05326},
  year={2015}
}

@article{ren2015faster,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={Medical Image Computing and Computer-Assisted Intervention--MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@inproceedings{harley2015evaluation,
  title={Evaluation of deep convolutional nets for document image classification and retrieval},
  author={Harley, Adam W and Ufkes, Alex and Derpanis, Konstantinos G},
  booktitle={2015 13th International Conference on Document Analysis and Recognition (ICDAR)},
  pages={991--995},
  year={2015},
  organization={IEEE}
}

@inproceedings{wang2015semantic,
  title={Semantic clustering and convolutional neural network for short text categorization},
  author={Wang, Peng and Xu, Jiaming and Xu, Bo and Liu, Chenglin and Zhang, Heng and Wang, Fangyuan and Hao, Hongwei},
  booktitle={Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
  pages={352--357},
  year={2015}
}

@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}

@article{pasupat2015compositional,
  title={Compositional semantic parsing on semi-structured tables},
  author={Pasupat, Panupong and Liang, Percy},
  journal={arXiv preprint arXiv:1508.00305},
  year={2015}
}

@inproceedings{nguyen2015relation,
  title={Relation extraction: Perspective from convolutional neural networks},
  author={Nguyen, Thien Huu and Grishman, Ralph},
  booktitle={Proceedings of the 1st workshop on vector space modeling for natural language processing},
  pages={39--48},
  year={2015}
}

@article{hermann2015teaching,
  title={Teaching machines to read and comprehend},
  author={Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{pham2016convolutional,
  title={Convolutional neural network language models},
  author={Pham, Ngoc-Quan and Kruszewski, German and Boleda, Gemma},
  booktitle={Proceedings of the 2016 conference on empirical methods in natural language processing},
  pages={1153--1162},
  year={2016}
}

@article{adel2016comparing,
  title={Comparing convolutional neural networks to traditional models for slot filling},
  author={Adel, Heike and Roth, Benjamin and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:1603.05157},
  year={2016}
}

@article{shazeer2016swivel,
  title={Swivel: Improving embeddings by noticing what's missing},
  author={Shazeer, Noam and Doherty, Ryan and Evans, Colin and Waterson, Chris},
  journal={arXiv preprint arXiv:1602.02215},
  year={2016}
}

@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@book{GoodfellowDL,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    year={2016}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@inproceedings{hao2016table,
  title={A table detection method for pdf documents based on convolutional neural networks},
  author={Hao, Leipeng and Gao, Liangcai and Yi, Xiaohan and Tang, Zhi},
  booktitle={2016 12th IAPR Workshop on Document Analysis Systems (DAS)},
  pages={287--292},
  year={2016},
  organization={IEEE}
}

@article{shazeer2016swivel,
  title={Swivel: Improving embeddings by noticing what's missing},
  author={Shazeer, Noam and Doherty, Ryan and Evans, Colin and Waterson, Chris},
  journal={arXiv preprint arXiv:1602.02215},
  year={2016}
}

@article{jozefowicz2016exploring,
  title={Exploring the limits of language modeling},
  author={Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
  journal={arXiv preprint arXiv:1602.02410},
  year={2016}
}

@article{bojanowski2017enriching,
  title={Enriching word vectors with subword information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={Transactions of the association for computational linguistics},
  volume={5},
  pages={135--146},
  year={2017},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@inproceedings{xie2017aggregated,
  title={Aggregated residual transformations for deep neural networks},
  author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1492--1500},
  year={2017}
}

@inproceedings{lin2017feature,
  title={Feature pyramid networks for object detection},
  author={Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2117--2125},
  year={2017}
}

@inproceedings{yang2017learning,
  title={Learning to extract semantic structure from documents using multimodal fully convolutional neural networks},
  author={Yang, Xiao and Yumer, Ersin and Asente, Paul and Kraley, Mike and Kifer, Daniel and Lee Giles, C},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5315--5324},
  year={2017}
}

@inproceedings{wehrmann2017character,
  title={A character-based convolutional neural network for language-agnostic Twitter sentiment analysis},
  author={Wehrmann, Joonatas and Becker, Willian and Cagnini, Henry EL and Barros, Rodrigo C},
  booktitle={2017 International Joint Conference on Neural Networks (IJCNN)},
  pages={2384--2391},
  year={2017},
  organization={IEEE}
}

@article{olive2017processing,
  title={Processing time and cognitive effort of longhand note taking when reading and summarizing a structured or linear text},
  author={Olive, Thierry and Barbier, Marie-Laure},
  journal={Written Communication},
  volume={34},
  number={2},
  pages={224--246},
  year={2017},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{paszke2017automatic,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}

@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}

@inproceedings{oliveira2018dhsegment,
  title={dhSegment: A generic deep-learning approach for document segmentation},
  author={Oliveira, Sofia Ares and Seguin, Benoit and Kaplan, Frederic},
  booktitle={2018 16th International Conference on Frontiers in Handwriting Recognition (ICFHR)},
  pages={7--12},
  year={2018},
  organization={IEEE}
}

@article{shaw2018self,
  title={Self-attention with relative position representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal={arXiv preprint arXiv:1803.02155},
  year={2018}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@article{conneau2018senteval,
  title={Senteval: An evaluation toolkit for universal sentence representations},
  author={Conneau, Alexis and Kiela, Douwe},
  journal={arXiv preprint arXiv:1803.05449},
  year={2018}
}

@article{omar2018arabic,
  title={Arabic nested noun compound extraction based on linguistic features and statistical measures},
  author={Omar, Nazlia and Al-Tashi, Qasem},
  journal={GEMA Online{\textregistered} Journal of Language Studies},
  volume={18},
  number={2},
  year={2018}
}

@article{trinh2018simple,
  title={A simple method for commonsense reasoning},
  author={Trinh, Trieu H and Le, Quoc V},
  journal={arXiv preprint arXiv:1806.02847},
  year={2018}
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

@article{katti2018chargrid,
  title={Chargrid: Towards understanding 2d documents},
  author={Katti, Anoop Raveendra and Reisswig, Christian and Guder, Cordula and Brarda, Sebastian and Bickel, Steffen and H{\"o}hne, Johannes and Faddoul, Jean Baptiste},
  journal={arXiv preprint arXiv:1809.08799},
  year={2018}
}

@article{fan2018hierarchical,
  title={Hierarchical neural story generation},
  author={Fan, Angela and Lewis, Mike and Dauphin, Yann},
  journal={arXiv preprint arXiv:1805.04833},
  year={2018}
}

@inproceedings{cho2018adversarial,
  title={Adversarial tableqa: Attention supervision for question answering on tables},
  author={Cho, Minseok and Amplayo, Reinald Kim and Hwang, Seung-won and Park, Jonghyuck},
  booktitle={Asian Conference on Machine Learning},
  pages={391--406},
  year={2018},
  organization={PMLR}
}

@article{huang2018music,
  title={Music transformer},
  author={Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Shazeer, Noam and Simon, Ian and Hawthorne, Curtis and Dai, Andrew M and Hoffman, Matthew D and Dinculescu, Monica and Eck, Douglas},
  journal={arXiv preprint arXiv:1809.04281},
  year={2018}
}

@article{cohan2018discourse,
  title={A discourse-aware attention model for abstractive summarization of long documents},
  author={Cohan, Arman and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Kim, Seokhwan and Chang, Walter and Goharian, Nazli},
  journal={arXiv preprint arXiv:1804.05685},
  year={2018}
}

@article{sharma2019bigpatent,
  title={BIGPATENT: A large-scale dataset for abstractive and coherent summarization},
  author={Sharma, Eva and Li, Chen and Wang, Lu},
  journal={arXiv preprint arXiv:1906.03741},
  year={2019}
}

@inproceedings{soto2019visual,
  title={Visual detection with context for document layout analysis},
  author={Soto, Carlos and Yoo, Shinjae},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={3464--3470},
  year={2019}
}

@inproceedings{duque2019squeezed,
  title={Squeezed very deep convolutional neural networks for text classification},
  author={Duque, Andr{\'e}a B and Santos, Lu{\~a} L{\'a}zaro J and Mac{\^e}do, David and Zanchettin, Cleber},
  booktitle={International Conference on Artificial Neural Networks},
  pages={193--207},
  year={2019},
  organization={Springer}
}

@article{denk2019bertgrid,
  title={Bertgrid: Contextualized embedding for 2d document representation and understanding},
  author={Denk, Timo I and Reisswig, Christian},
  journal={arXiv preprint arXiv:1909.04948},
  year={2019}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{liu2019graph,
  title={Graph convolution for multimodal information extraction from visually rich documents},
  author={Liu, Xiaojing and Gao, Feiyu and Zhang, Qiong and Zhao, Huasha},
  journal={arXiv preprint arXiv:1903.11279},
  year={2019}
}

@inproceedings{huang2019icdar2019,
  title={Icdar2019 competition on scanned receipt ocr and information extraction},
  author={Huang, Zheng and Chen, Kai and He, Jianhua and Bai, Xiang and Karatzas, Dimosthenis and Lu, Shijian and Jawahar, CV},
  booktitle={2019 International Conference on Document Analysis and Recognition (ICDAR)},
  pages={1516--1520},
  year={2019},
  organization={IEEE}
}

@inproceedings{park2019cord,
  title={CORD: a consolidated receipt dataset for post-OCR parsing},
  author={Park, Seunghyun and Shin, Seung and Lee, Bado and Lee, Junyeop and Surh, Jaeheung and Seo, Minjoon and Lee, Hwalsuk},
  booktitle={Workshop on Document Intelligence at NeurIPS 2019},
  year={2019}
}

@inproceedings{jaume2019funsd,
  title={Funsd: A dataset for form understanding in noisy scanned documents},
  author={Jaume, Guillaume and Ekenel, Hazim Kemal and Thiran, Jean-Philippe},
  booktitle={2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)},
  volume={2},
  pages={1--6},
  year={2019},
  organization={IEEE}
}

@inproceedings{zhong2019publaynet,
  title={Publaynet: largest dataset ever for document layout analysis},
  author={Zhong, Xu and Tang, Jianbin and Yepes, Antonio Jimeno},
  booktitle={2019 International Conference on Document Analysis and Recognition (ICDAR)},
  pages={1015--1022},
  year={2019},
  organization={IEEE}
}

@inproceedings{zellers2019defending,
  title={Defending against neural fake news},
  author={Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9054--9065},
  year={2019}
}

@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{luo2019reading,
  title={Reading like HER: Human reading inspired extractive summarization},
  author={Luo, Ling and Ao, Xiang and Song, Yan and Pan, Feiyang and Yang, Min and He, Qing},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={3033--3043},
  year={2019}
}

@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@article{rae2019compressive,
  title={Compressive transformers for long-range sequence modelling},
  author={Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Lillicrap, Timothy P},
  journal={arXiv preprint arXiv:1911.05507},
  year={2019}
}

@inproceedings{xu2020layoutlm,
  title={Layoutlm: Pre-training of text and layout for document image understanding},
  author={Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={1192--1200},
  year={2020}
}

@article{powalski2020unicase,
  title={UniCase--Rethinking Casing in Language Models},
  author={Powalski, Rafal and Stanislawek, Tomasz},
  journal={arXiv preprint arXiv:2010.11936},
  year={2020}
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{kitaev2020reformer,
  title={Reformer: The Efficient Transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{tay2020efficient,
    author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
    title={Efficient Transformers: A Survey},
    journal={arXiv preprint arXiv:2009.06732},
    year={2020} 
}

@misc{arxiv2020,
    author={arXiv},
    year={2020},
    title={arXiv Bulk Data Access},
    note={\url{https://arxiv.org/help/bulk_data}} 
}

@article{scialom2020mlsum,
  title={MLSUM: The multilingual summarization corpus},
  author={Scialom, Thomas and Dray, Paul-Alexis and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo},
  journal={arXiv preprint arXiv:2004.14900},
  year={2020}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{ke2020rethinking,
  title={Rethinking positional encoding in language pre-training},
  author={Ke, Guolin and He, Di and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2006.15595},
  year={2020}
}

@article{liu2020multichannel,
  title={Multichannel cnn with attention for text classification},
  author={Liu, Zhenyu and Huang, Haiwei and Lu, Chaohong and Lyu, Shengfei},
  journal={arXiv preprint arXiv:2006.16174},
  year={2020}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{li2020docbank,
  title={DocBank: A benchmark dataset for document layout analysis},
  author={Li, Minghao and Xu, Yiheng and Cui, Lei and Huang, Shaohan and Wei, Furu and Li, Zhoujun and Zhou, Ming},
  journal={arXiv preprint arXiv:2006.01038},
  year={2020}
}

@article{gralinski2020kleister,
  title={Kleister: A novel task for information extraction involving long documents with complex layout},
  author={Grali{\'n}ski, Filip and Stanis{\l}awek, Tomasz and Wr{\'o}blewska, Anna and Lipi{\'n}ski, Dawid and Kaliska, Agnieszka and Rosalska, Paulina and Topolski, Bartosz and Biecek, Przemys{\l}aw},
  journal={arXiv preprint arXiv:2003.02356},
  year={2020}
}

@article{liu2020multilingual,
  title={Multilingual denoising pre-training for neural machine translation},
  author={Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={726--742},
  year={2020},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@article{wang2020linformer,
  title={Linformer: Self-Attention with Linear Complexity},
  author={Wang, Sinong and Li, Belinda and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}

@article{pramanik2020towards,
  title={Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning},
  author={Pramanik, Subhojeet and Mujumdar, Shashank and Patel, Hima},
  journal={arXiv preprint arXiv:2009.14457},
  year={2020}
}


@article{tay2020long,
  title={Long Range Arena: A Benchmark for Efficient Transformers},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  journal={arXiv preprint arXiv:2011.04006},
  year={2020}
}

@article{ainslie2020etc,
  title={ETC: Encoding long and structured inputs in transformers},
  author={Ainslie, Joshua and Ontanon, Santiago and Alberti, Chris and Cvicek, Vaclav and Fisher, Zachary and Pham, Philip and Ravula, Anirudh and Sanghai, Sumit and Wang, Qifan and Yang, Li},
  journal={arXiv preprint arXiv:2004.08483},
  year={2020}
}

@inproceedings{zhang2020pegasus,
  title={Pegasus: Pre-training with extracted gap-sentences for abstractive summarization},
  author={Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
  booktitle={International Conference on Machine Learning},
  pages={11328--11339},
  year={2020},
  organization={PMLR}
}

@article{xu2020layoutlmv2,
  title={LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding},
  author={Xu, Yang and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wei, Furu and Wang, Guoxin and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Che, Wanxiang and others},
  journal={arXiv preprint arXiv:2012.14740},
  year={2020}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{cho2020x,
  title={X-lxmert: Paint, caption and answer questions with multi-modal transformers},
  author={Cho, Jaemin and Lu, Jiasen and Schwenk, Dustin and Hajishirzi, Hannaneh and Kembhavi, Aniruddha},
  journal={arXiv preprint arXiv:2009.11278},
  year={2020}
}

@article{sun2021long,
  title={Do long-range language models actually use long-range context?},
  author={Sun, Simeng and Krishna, Kalpesh and Mattarella-Micke, Andrew and Iyyer, Mohit},
  journal={arXiv preprint arXiv:2109.09115},
  year={2021}
}

@article{gehrmann2021gem,
  title={The gem benchmark: Natural language generation, its evaluation and metrics},
  author={Gehrmann, Sebastian and Adewumi, Tosin and Aggarwal, Karmanya and Ammanamanchi, Pawan Sasanka and Anuoluwapo, Aremu and Bosselut, Antoine and Chandu, Khyathi Raghavi and Clinciu, Miruna and Das, Dipanjan and Dhole, Kaustubh D and others},
  journal={arXiv preprint arXiv:2102.01672},
  year={2021}
}

@article{wu2021lampret,
  title={Lampret: Layout-aware multimodal pretraining for document understanding},
  author={Wu, Te-Lin and Li, Cheng and Zhang, Mingyang and Chen, Tao and Hombaiah, Spurthi Amba and Bendersky, Michael},
  journal={arXiv preprint arXiv:2104.08405},
  year={2021}
}

@inproceedings{li2021selfdoc,
  title={Selfdoc: Self-supervised document representation learning},
  author={Li, Peizhao and Gu, Jiuxiang and Kuen, Jason and Morariu, Vlad I and Zhao, Handong and Jain, Rajiv and Manjunatha, Varun and Liu, Hongfu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5652--5660},
  year={2021}
}

@inproceedings{mathew2021docvqa,
  title={Docvqa: A dataset for vqa on document images},
  author={Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={2200--2209},
  year={2021}
}

@article{tang2020multilingual,
  title={Multilingual translation with extensible multilingual pretraining and finetuning},
  author={Tang, Yuqing and Tran, Chau and Li, Xian and Chen, Peng-Jen and Goyal, Naman and Chaudhary, Vishrav and Gu, Jiatao and Fan, Angela},
  journal={arXiv preprint arXiv:2008.00401},
  year={2020}
}

@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={8821--8831},
  year={2021},
  organization={PMLR}
}

@article{powalski2021going,
  title={Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer},
  author={Powalski, Rafa{\l} and Borchmann, {\L}ukasz and Jurkiewicz, Dawid and Dwojak, Tomasz and Pietruszka, Micha{\l} and Pa{\l}ka, Gabriela},
  journal={arXiv preprint arXiv:2102.09550},
  year={2021}
}

@article{appalaraju2021docformer,
  title={DocFormer: End-to-End Transformer for Document Understanding},
  author={Appalaraju, Srikar and Jasani, Bhavan and Kota, Bhargava Urala and Xie, Yusheng and Manmatha, R},
  journal={arXiv preprint arXiv:2106.11539},
  year={2021}
}

@inproceedings{nguyen-etal-2021-skim-attention,
    title = "Skim-Attention: Learning to Focus via Document Layout",
    author = "Nguyen, Laura  and
      Scialom, Thomas  and
      Staiano, Jacopo  and
      Piwowarski, Benjamin",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.207",
    doi = "10.18653/v1/2021.findings-emnlp.207",
    pages = "2413--2427",
    abstract = "Transformer-based pre-training techniques of text and layout have proven effective in a number of document understanding tasks. Despite this success, multimodal pre-training models suffer from very high computational and memory costs. Motivated by human reading strategies, this paper presents Skim-Attention, a new attention mechanism that takes advantage of the structure of the document and its layout. Skim-Attention only attends to the 2-dimensional position of the words in a document. Our experiments show that Skim-Attention obtains a lower perplexity than prior works, while being more computationally efficient. Skim-Attention can be further combined with long-range Transformers to efficiently process long documents. We also show how Skim-Attention can be used off-the-shelf as a mask for any Pre-trained Language Model, allowing to improve their performance while restricting attention. Finally, we show the emergence of a document structure representation in Skim-Attention.",
}

@inproceedings{mathew2022infographicvqa,
  title={InfographicVQA},
  author={Mathew, Minesh and Bagal, Viraj and Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1697--1706},
  year={2022}
}

@inproceedings{borchmann2021due,
  title={DUE: End-to-End Document Understanding Benchmark},
  author={Borchmann, {\L}ukasz and Pietruszka, Micha{\l} and Stanislawek, Tomasz and Jurkiewicz, Dawid and Turski, Micha{\l} and Szyndler, Karolina and Grali{\'n}ski, Filip},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
  year={2021}
}

@article{chaudhari2021attentive,
  title={An attentive survey of attention models},
  author={Chaudhari, Sneha and Mithal, Varun and Polatkan, Gungor and Ramanath, Rohan},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={12},
  number={5},
  pages={1--32},
  year={2021},
  publisher={ACM New York, NY}
}

@article{shen2021layoutparser,
  title={LayoutParser: A Unified Toolkit for Deep Learning Based Document Image Analysis},
  author={Shen, Zejiang and Zhang, Ruochen and Dell, Melissa and Lee, Benjamin Charles Germain and Carlson, Jacob and Li, Weining},
  journal={arXiv preprint arXiv:2103.15348},
  year={2021}
}

@inproceedings{huang2022layoutlmv3,
  title={Layoutlmv3: Pre-training for document ai with unified text and image masking},
  author={Huang, Yupan and Lv, Tengchao and Cui, Lei and Lu, Yutong and Wei, Furu},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={4083--4091},
  year={2022}
}

@article{qin2022nlp,
  title={The nlp task effectiveness of long-range transformers},
  author={Qin, Guanghui and Feng, Yukun and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:2202.07856},
  year={2022}
}

@article{peng2022ernie,
  title={Ernie-layout: Layout knowledge enhanced pre-training for visually-rich document understanding},
  author={Peng, Qiming and Pan, Yinxu and Wang, Wenjin and Luo, Bin and Zhang, Zhenyu and Huang, Zhengjie and Hu, Teng and Yin, Weichong and Chen, Yongfeng and Zhang, Yin and others},
  journal={arXiv preprint arXiv:2210.06155},
  year={2022}
}

@article{lee2022formnet,
  title={Formnet: Structural encoding beyond sequential modeling in form document information extraction},
  author={Lee, Chen-Yu and Li, Chun-Liang and Dozat, Timothy and Perot, Vincent and Su, Guolong and Hua, Nan and Ainslie, Joshua and Wang, Renshen and Fujii, Yasuhisa and Pfister, Tomas},
  journal={arXiv preprint arXiv:2203.08411},
  year={2022}
}

@article{miculicich2022document,
  title={Document Summarization with Text Segmentation},
  author={Miculicich, Lesly and Han, Benjamin},
  year={2022}
}

@article{cao2022hibrids,
  title={Hibrids: Attention with hierarchical biases for structure-aware long document summarization},
  author={Cao, Shuyang and Wang, Lu},
  journal={arXiv preprint arXiv:2203.10741},
  year={2022}
}

@inproceedings{kovavcevic2022bidirectional,
  title={Bidirectional LSTM networks for abstractive text summarization},
  author={Kova{\v{c}}evi{\'c}, Aldin and Ke{\v{c}}o, Dino},
  booktitle={Advanced Technologies, Systems, and Applications VI: Proceedings of the International Symposium on Innovative and Interdisciplinary Applications of Advanced Technologies (IAT) 2021},
  pages={281--293},
  year={2022},
  organization={Springer}
}

@inproceedings{nguyen-etal-2023-loralay,
    title = "{L}o{R}a{L}ay: A Multilingual and Multimodal Dataset for Long Range and Layout-Aware Summarization",
    author = "Nguyen, Laura  and
      Scialom, Thomas  and
      Piwowarski, Benjamin  and
      Staiano, Jacopo",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.46",
    pages = "636--651",
    abstract = "Text Summarization is a popular task and an active area of research for the Natural Language Processing community. By definition, it requires to account for long input texts, a characteristic which poses computational challenges for neural models. Moreover, real-world documents come in a variety of complex, visually-rich, layouts. This information is of great relevance, whether to highlight salient content or to encode long-range interactions between textual passages. Yet, all publicly available summarization datasets only provide plain text content. To facilitate research on how to exploit visual/layout information to better capture long-range dependencies in summarization models, we present LoRaLay, a collection of datasets for long-range summarization with accompanying visual/layout information. We extend existing and popular English datasets (arXiv and PubMed) with layout information and propose four novel datasets {--} consistently built from scholar resources {--} covering French, Spanish, Portuguese, and Korean languages. Further, we propose new baselines merging layout-aware and long-range models {--} two orthogonal approaches {--} and obtain state-of-the-art results, showing the importance of combining both lines of research.",
}

@article{thirunavukarasu2023large,
  title={Large language models in medicine},
  author={Thirunavukarasu, Arun James and Ting, Darren Shu Jeng and Elangovan, Kabilan and Gutierrez, Laura and Tan, Ting Fang and Ting, Daniel Shu Wei},
  journal={Nature medicine},
  pages={1--11},
  year={2023},
  publisher={Nature Publishing Group US New York}
}

@article{wang2023scientific,
  title={Scientific discovery in the age of artificial intelligence},
  author={Wang, Hanchen and Fu, Tianfan and Du, Yuanqi and Gao, Wenhao and Huang, Kexin and Liu, Ziming and Chandak, Payal and Liu, Shengchao and Van Katwyk, Peter and Deac, Andreea and others},
  journal={Nature},
  volume={620},
  number={7972},
  pages={47--60},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{xu2022systematic,
  title={A systematic evaluation of large language models of code},
  author={Xu, Frank F and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent Josua},
  booktitle={Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},
  pages={1--10},
  year={2022}
}