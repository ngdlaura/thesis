
\chapter{Conclusion}
\label{chapter:conclusion}

\acresetall
\acused{SHADE}

%\minitoc

This thesis explores the field of Document Understanding through the application of Deep Learning techniques. Our exploration is guided by two main research axes. First, we delve into the challenge of efficiently and effectively encoding the multimodal nature of documents, extending beyond textual content to consider the complex interplay of layout and visual elements. Secondly, we address the challenge of efficiently processing long and complex documents by leveraging their layout.

\section{Summary of Contributions}

In the past decade, Deep Learning techniques, in particular Foundation Models \citep{devlin2018bert, radford2019language, touvron2023llama} built upon the Transformer architecture \citep{vaswani2017attention}, have dominated the field of \ac{NLP}. This trend has prompted a paradigm shift in the field of Document Understanding, witnessing a significant rise in the adoption of large-scale, general-purpose multimodal Pre-trained Language Models. LayoutLM \citep{xu2020layoutlm} initiated the use of Transformers' modeling capabilities to jointly pre-train text and layout information for a range of tasks involving visually-rich documents. Our first research axis involved contributing to this shift towards integrating document layout into pre-training by developing novel strategies for encoding layout information efficiently (Chapter~\ref{chapter:skim-attention}) and enhancing robustness on documents with unreliable reading order information (Chapter~\ref{chapter:layout2pos}). Subsequently, various approaches have investigated the extension of multimodal Pre-trained Language Models to longer, more complex documents. Our second research axis centered on leveraging layout information to capture long-range dependencies (Chapter~\ref{chapter:loralay}), capitalizing on its potential to reduce the complexity of self-attention mechanisms (Chapter~\ref{chapter:skim-attention}). We will now outline the main contributions derived from our work.

% It has emerged as the building block for developing more complex document understanding systems, leading to state-of-the-art performance across various document understanding tasks.

% It has emerged as the building block for designing more complex document understanding systems that incorporate visual appearance through Computer Vision techniques \citep{xu2020layoutlmv2, appalaraju2021docformer, li2021selfdoc}, employ advanced strategies for integrating layout information \citep{wu2021lampret, lee2022formnet, peng2022ernie}, and use novel model architectures and pre-training tasks to better absorb cross-modal knowledge \citep{kim2022ocr, powalski2021going, hong2020bros, peng2022ernie}.

\paragraph{Literature review}

In Chapters~\ref{chapter:related-language-modeling} to~\ref{chapter:related-document-understanding}, we conducted a thorough literature review, extensively exploring works relevant to our research. Chapter~\ref{chapter:related-language-modeling} delves into the foundational aspects of language modeling, with a specific emphasis on Neural Language Models. In Chapter~\ref{chapter:related-pretrained-language-models}, we provided a detailed explanation of the Transformer architecture and its core component—the attention mechanism. Additionally, we explored the use of Transformers to construct powerful language models, with a focus on key Pre-trained Language Models that underpin recent advancements in Document Understanding. Transitioning to longer documents in Chapter~\ref{chapter:related-long-range-modeling}, we consolidated and organized efficient self-attention model variants designed to reduce the computational cost of Transformers, enabling efficient modeling of long texts. In Chapter~\ref{chapter:related-document-understanding}, we reviewed representative tasks and benchmark datasets within the field of Document Understanding. We then structured and reviewed Deep Learning approaches, with a specific focus on multimodal pre-training techniques, which serve as the cornerstone for our contributions.


\paragraph{Layout-based attention strategies}

In Chapter~\ref{chapter:skim-attention}, we tackled the efficiency challenges of multimodal pre-training techniques by strategically leveraging layout. Inspired by human reading strategies, we introduced \textit{Skim-Attention}, a novel attention mechanism based on layout information exclusively. Addressing key research questions, we illustrated Skim-Attention's ability to determine attention solely from layout and reduce the complexity of self-attention. Furthermore, we demonstrated its versatility as a mask for any Pre-trained Language Model, enhancing performance while sparsifying attention. Lastly, we showed the emergence of a document structure representation within Skim-Attention.

\paragraph{Leveraging layout to mitigate reading order issues}

In Chapter~\ref{chapter:layout2pos}, we addressed challenges in document understanding tasks arising from inaccuracies caused by \ac{OCR} in determining the reading order of visually-rich documents. To overcome these challenges, we introduced \textit{Layout2Pos}—a shallow Transformer that generates position embeddings from the spatial arrangement of text, avoiding reading order issues. Integrated into a \ac{BART} model \citep{lewis2019bart} architecture, Layout2Pos competes with models relying on reading order across three benchmark datasets for visual information extraction. Importantly, we showcased substantial performance drops in models dependent on reading order when evaluated with a different reading order than seen during training. This emphasizes the significance of not relying on \ac{OCR}-induced reading orders. 

\paragraph{Leveraging layout to deal with long and layout-rich documents}

In Chapter~\ref{chapter:loralay}, we extended the exploration of layout-based techniques introduced in Chapters~\ref{chapter:skim-attention} and~\ref{chapter:layout2pos} to summarization of long documents. We introduced \textit{LoRaLay}, a collection of datasets for long-range summarization with visual/layout information, addressing the lack of multimodal information in existing summarization datasets. We augmented popular English datasets and built new datasets for French, Spanish, Portuguese, and Korean. Baselines merging layout-aware and long-range models were introduced, achieving state-of-the-art results and highlighting the importance of considering both research directions for summarization of long documents. Additionally, we proposed an annotation interface for human evaluation and a novel metric to assess information flow preservation in generated summaries.

% Limits: we did not explore use of visual information

\section{Perspectives}

\paragraph{Towards universal end-to-end document understanding systems}

Propelled by \ac{T5} \citep{raffel2020exploring}, research into unifying training processes across diverse tasks and domains using the sequence-to-sequence framework has made remarkable advancements. Leveraging the same framework, our contributions in Chapter~\ref{chapter:layout2pos} and~\ref{chapter:loralay} align with this research trajectory and present opportunities for further improvement through unified training approaches. In the field of Document Understanding, unified training has gained traction with the development of generative language models such as TILT \citep{powalski2021going}, partly pre-trained in a supervised manner using various tasks related to visually-rich documents, and UDOP \citep{tang2023unifying}, which unifies representations for image, text, and layout while employing self-supervised tasks across all modalities. Another noteworthy direction in advancing end-to-end document understanding systems involves \ac{OCR}-free models, exemplified by Donut \citep{kim2022ocr}. Donut employs a high-resolution image encoder during pre-training to detect text, thereby establishing a direct mapping from raw input images to the desired outputs. As such, a promising perspective for future research involves the development of \ac{OCR}-free end-to-end frameworks that unify all modalities and task paradigms, laying the groundwork for foundation document understanding models.

\paragraph{Leveraging Large Language Models}

Large Language Models have transformed the landscape of \ac{NLP} by showcasing remarkable abilities to address tasks based on minimal instructions \citep{wei2021finetuned} or a small number of examples integrated into the prompt \citep{brown2020language} across diverse linguistic applications. Exploration of Large Language Models for multimodal generation has gained traction recently. LLaVA \citep{liu2023visual}, and mPLUG-Owl \citep{ye2023mplug} leverage Large Language Models to construct unified end-to-end multimodal models tailored for processing images that contain text (\textit{e.g.}, book covers and movie posters). Despite these achievements, successfully applying Large Language Models to the domain of Document Understanding, where text is dense and visually-situated, remains a challenge primarily due to the lack of in-domain training. Specifically, these models struggle with tasks such as handling diverse image types, recognizing complex textual content, and comprehending the relationships between visual semantics and textual information. With the rise of prominent efforts to improve Large Language Models for document understanding tasks, such as mPLUG-DocOwl \citep{ye2023mplug} and LMDX \citep{perot2023lmdx}, a natural perspective for future research involves developing stronger document understanding abilities within Large Language Models.

% This is primarily due to the lack of layout encoding within Large Language Models and the absence of a grounding mechanism that ensures the generated text remains free from hallucination.

% Large Language Models have revolutionized Natural Language Processing, showing the capabilities to solve a task with simply an instruction (Wei et al., 2022) or a few examples added to the prompt (Brown et al., 2020). 
% Large Language Models  have demonstrated impressive zero-shot abilities across various linguistic applications.
% The main obstacles to LLM adoption in the field have been the absence of layout encoding within LLMs, critical for a high quality extraction, and the lack of a grounding mechanism ensuring the answer is not hallucinated.
%  Recent research has also explored the application of LLMs for multi-modal generation.  MiniGPT-4 [Zhu et al., 2023], LLaVA [Liu et al., 2023a], and mPLUG-Owl [Ye et al., 2023], leverage LLMs to build unified models for multi-modality with limited connected parameters. These methods show superficial OCR-free text recognition abilities under the zero-shot setting. However, for complicated document understanding, due to lacking in-domain training, they encounter challenges in handling diverse image types, recognizing rich texts and comprehending relationships between visual semantic and text information. 
% Nevertheless, without in-domain training, these models tend to ignore fine-grained OCR features, such as sophisticated tables or large blocks of text, which are essential for OCR-free document understanding. I