
\chapter{Conclusion}
\label{chapter:conclusion}

\acresetall
\acused{SHADE}

%\minitoc

This thesis explores the field of Document Understanding in the context of Deep Learning. Although Deep Learning has made notable strides in enhancing document understanding systems, the field still encounters various challenges in real-world applications, including limitations in handling long documents, disparities between high-quality training data and real-world documents, resource constraints, and dependence on \ac{OCR} systems.

Our exploration is guided by two main research axes. First, we delve into the challenge of efficiently and effectively encoding the multimodal nature of documents, extending beyond textual content to consider the complex interplay of layout and visual elements. Secondly, we address the challenge of efficiently and effectively processing long and complex documents while leveraging their layout.

\section{Summary of Contributions}

In the past decade, Deep Learning techniques, in particular Foundation Models \citep{devlin2018bert, radford2019language, touvron2023llama} built upon the Transformer architecture \citep{vaswani2017attention}, have dominated the field of \ac{NLP}. This trend has prompted a paradigm shift in the field of Document Understanding, witnessing a significant rise in the adoption of large-scale, general-purpose multimodal Pre-trained Language Models. LayoutLM \citep{xu2020layoutlm} initiated the use of Transformers' modeling capabilities to jointly pre-train text and layout information for a range of tasks involving visually-rich documents. 

Our first research axis involved contributing to this shift towards integrating document layout into pre-training by developing novel strategies for encoding layout information efficiently (Chapter~\ref{chapter:skim-attention}) and enhancing robustness on documents with unreliable reading order information (Chapter~\ref{chapter:layout2pos}). 

Various approaches have investigated the extension of multimodal Pre-trained Language Models to longer, more complex documents. Our second research axis centered on leveraging layout information to capture long-range dependencies (Chapter~\ref{chapter:loralay}), capitalizing on its potential to reduce the complexity of self-attention mechanisms (Chapter~\ref{chapter:skim-attention}). We will now outline the main contributions derived from our work.

% It has emerged as the building block for developing more complex document understanding systems, leading to state-of-the-art performance across various document understanding tasks.

% It has emerged as the building block for designing more complex document understanding systems that incorporate visual appearance through Computer Vision techniques \citep{xu2020layoutlmv2, appalaraju2021docformer, li2021selfdoc}, employ advanced strategies for integrating layout information \citep{wu2021lampret, lee2022formnet, peng2022ernie}, and use novel model architectures and pre-training tasks to better absorb cross-modal knowledge \citep{kim2022ocr, powalski2021going, hong2020bros, peng2022ernie}.

\paragraph{Literature review}

In Chapters~\ref{chapter:related-language-modeling} to~\ref{chapter:related-document-understanding}, we conducted a thorough literature review of works relevant to our research. Chapter~\ref{chapter:related-language-modeling} focused on foundational aspects of language modeling, particularly Neural Language Models. In Chapter~\ref{chapter:related-pretrained-language-models}, we detailed the Transformer architecture and its core component—the attention mechanism. We explored the use of Transformers to construct key Pre-trained Language Models, before focusing on efficient self-attention model variants for handling long texts. In Chapter~\ref{chapter:related-document-understanding}, we examined tasks, datasets, and Deep Learning approaches in Document Understanding, emphasizing multimodal pre-training techniques pivotal to our contributions.
 
\paragraph{Layout-based attention strategies}

While prior works typically consider layout as a mere positional feature, failing to fully capitalize on the strong correlation between text and layout, we took a different approach in Chapter~\ref{chapter:skim-attention}. Moving beyond this conventional view of layout, we drew inspiration from human reading strategies to effectively leverage layout and tackle the efficiency challenges of multimodal pre-training techniques. We introduced \textit{Skim-Attention}, a novel attention mechanism based on layout information exclusively. Addressing key research questions, we illustrated Skim-Attention's ability to determine attention solely from layout (Skimformer), which can potentially be used to reduce the complexity of self-attention. Furthermore, we demonstrated that Skim-Attention can also be used as a mask for any Pre-trained Language Model (Skimming Mask), enhancing performance while sparsifying attention. Lastly, we showed the emergence of a document structure representation within Skim-Attention. In summary, by strategically integrating layout into language modeling, Skim-Attention marks a pioneering effort in leveraging layout to alleviate the computational burden associated with self-attention.

While improving efficiency, determining attention solely from layout might overlook crucial semantic clues essential for the model's comprehension. While Skimming Mask addresses this issue, the masking pattern should be learned in an end-to-end fashion alongside the Transformer model for full adaptability. Therefore, potential extensions of this work include the implementation of an end-to-end version of Skimming Mask. Given the discrete nature of attention masks, reinforcement learning algorithms could be explored. Another research direction involves the application of Skim-Attention to tasks that require capturing longer-range dependencies. 

\paragraph{Leveraging layout to avoid reading order issues}

The majority of pre-training methods for Document Understanding rely on serialized text. However, \ac{OCR} engines often struggle to accurately determine reading orders for documents with complex layouts, thereby impacting the entire text processing pipeline. In contrast, layout inherently organizes content visually, providing the correct reading order for documents. In Chapter~\ref{chapter:layout2pos}, we diverged from conventional approaches by relying solely on layout information to offer an alternative for the provided reading order of documents. We introduced \textit{Layout2Pos}—a shallow Transformer that generates position embeddings from the spatial arrangement of text, avoiding reading order issues. Integrated into a \ac{BART} model \citep{lewis2019bart} architecture, Layout2Pos competes with models relying on reading order across three benchmark datasets for visual information extraction. Importantly, we showcased substantial performance drops in models dependent on reading order when evaluated with a different reading order than seen during training. This emphasizes the significance of not relying on \ac{OCR}-induced reading orders. Overall, Layout2Pos marks the pioneering use of layout as a method to provide reading order information, completely eliminating reliance on \ac{OCR}-induced reading orders for downstream tasks. 

To further enhance the model, several avenues for improvement can be explored. Layout information could be better captured, for instance, by incorporating details such as page numbers. Integrating visual information could also help generate position embeddings that better align with the reading order of documents. Moreover, to improve generalizability, another potential extension of this work involves focusing on more complex datasets containing longer documents and encompassing languages beyond English. Furthermore, in our sequence-to-sequence evaluation framework, any organization of key-value pairs is deemed valid. However, the use of teacher forcing during training tends to prioritize a single correct output, potentially penalizing valid responses with different entity orders. Therefore, future work involves exploring permutation invariant losses to enhance robustness. 

\paragraph{Leveraging layout to deal with long and layout-rich documents}

Prior research in cognitive sciences, particularly in the context of long documents, has showcased the need for efficient modeling of layout information in information processing. However, methods that address efficient understanding of long documents with awareness of their layouts are still largely underexplored. In Chapter~\ref{chapter:loralay}, we extended the exploration of layout-based techniques introduced in Chapters~\ref{chapter:skim-attention} and~\ref{chapter:layout2pos} to long documents. We focused on the Text Summarization task, given its reliance on document structures to guide the generation of summaries and its natural suitability to benefit from a global context. We introduced \textit{LoRaLay}, the first collection of datasets for long-range summarization with visual/layout information, addressing the lack of multimodal information in existing summarization datasets. We augmented popular English datasets with layout/visual information and built new datasets for French, Spanish, Portuguese, and Korean. Baselines combining layout-aware and long-range models were introduced, achieving state-of-the-art results and highlighting the importance of considering both research directions for summarization of long documents. Additionally, we proposed an annotation interface for human evaluation and a novel metric to assess information flow preservation in generated summaries. By bridging a significant gap in existing resources, LoRaLay provides a foundation for advancing our understanding of the role of layout in handling long textual content, propelling progress in developing efficient and effective layout-aware long-range approaches.

Future extensions of this work could involve developing vision-aware models, further expanding the scope of multimodal considerations in the context of long document summarization. Additionally, future work could include the development of more advanced models that strategically leverage layout to efficiently and effectively process long documents.

% Limits: we did not explore use of visual information

\section{Perspectives}

\paragraph{Towards universal end-to-end document understanding systems}

% Propelled by \ac{T5} \citep{raffel2020exploring}, research into unifying training processes across diverse tasks and domains using the sequence-to-sequence framework has made remarkable advancements. Leveraging the same framework, our generative language models introduced in Chapter~\ref{chapter:layout2pos} and~\ref{chapter:loralay} align with this research trajectory and present opportunities for further improvement through unified training approaches.

A possible direction involves the development of unified foundation document understanding models. Propelled by \ac{T5} \citep{raffel2020exploring}, research into unifying training processes across diverse tasks and domains using the sequence-to-sequence framework has made remarkable advancements. In the field of Document Understanding, unified training has also gained traction with the development of generative language models. As detailed in Section~\ref{section:related-document-understanding-pretraining-datasets}, TILT \citep{powalski2021going} is partly pre-trained in a supervised manner using various tasks related to visually-rich documents and unified under the sequence-to-sequence framework, whereas UDOP \citep{tang2023unifying} unifies representations for image, text, and layout while employing self-supervised tasks across all modalities. Leveraging the same sequence-to-sequence framework, our generative language models introduced in Chapter~\ref{chapter:layout2pos} and~\ref{chapter:loralay} align with this research trajectory and present opportunities for further improvement through unified training approaches.

While our generative Pre-trained Language Model proposed in Chapter~\ref{chapter:layout2pos} does not rely on sequential position information, it still employs text extracted through \ac{OCR}, introducing the possibility of inaccuracies and error propagation to the document understanding model. Another noteworthy direction in advancing end-to-end document understanding systems involves \ac{OCR}-free models, exemplified by Donut \citep{kim2022ocr}. Donut employs a high-resolution image encoder during pre-training to detect text, thereby establishing a direct mapping from raw input images to the desired outputs. As such, a perspective for future research involves the development of \ac{OCR}-free end-to-end frameworks that unify all modalities and task paradigms, laying the groundwork for foundation document understanding models. Nevertheless, it is worth nothing that the increasing prevalence of digital-born documents may reduce the necessity and importance of \ac{OCR} in the long term. Consequently, our work in this PhD, which still depends on text extracted through another system (\textit{e.g.}, PDF structured information), represents an important research avenue.

\paragraph{Leveraging Large Language Models}

The contributions in this PhD focus on Pre-trained Language Models, which necessitate specific fine-tuning on diverse downstream datasets to achieve optimal performance and fall short under the zero-shot setting. In contrast, the next iteration in Language Models, Large Language Models, has transformed the landscape of \ac{NLP} by showcasing remarkable abilities to address tasks based on minimal instructions \citep{wei2021finetuned} or a small number of examples integrated into the prompt \citep{brown2020language} across diverse linguistic applications. Exploration of Large Language Models for multimodal generation has gained traction recently. LLaVA \citep{liu2023visual}, mPLUG-Owl \citep{ye2023mplug}, and IDEFICS \citep{laurenccon2023obelisc} leverage Large Language Models to construct unified end-to-end multimodal models tailored for processing images that contain text (\textit{e.g.}, book covers and movie posters). Despite these achievements, successfully applying Large Language Models to the domain of Document Understanding, where text is dense and visually-situated, remains a challenge primarily due to the lack of in-domain training. Specifically, these models struggle with tasks such as handling diverse image types, recognizing complex textual content, and comprehending the relationships between visual semantics and textual information. With the rise of prominent efforts to improve Large Language Models for document understanding tasks, such as mPLUG-DocOwl \citep{ye2023mplug}, LMDX \citep{perot2023lmdx}, and DocLLM \citep{wang2023docllm}, a natural perspective for future research involves developing stronger document understanding abilities within Large Language Models.

% This is primarily due to the lack of layout encoding within Large Language Models and the absence of a grounding mechanism that ensures the generated text remains free from hallucination.

% Large Language Models have revolutionized Natural Language Processing, showing the capabilities to solve a task with simply an instruction (Wei et al., 2022) or a few examples added to the prompt (Brown et al., 2020). 
% Large Language Models  have demonstrated impressive zero-shot abilities across various linguistic applications.
% The main obstacles to LLM adoption in the field have been the absence of layout encoding within LLMs, critical for a high quality extraction, and the lack of a grounding mechanism ensuring the answer is not hallucinated.
%  Recent research has also explored the application of LLMs for multi-modal generation.  MiniGPT-4 [Zhu et al., 2023], LLaVA [Liu et al., 2023a], and mPLUG-Owl [Ye et al., 2023], leverage LLMs to build unified models for multi-modality with limited connected parameters. These methods show superficial OCR-free text recognition abilities under the zero-shot setting. However, for complicated document understanding, due to lacking in-domain training, they encounter challenges in handling diverse image types, recognizing rich texts and comprehending relationships between visual semantic and text information. 
% Nevertheless, without in-domain training, these models tend to ignore fine-grained OCR features, such as sophisticated tables or large blocks of text, which are essential for OCR-free document understanding. I